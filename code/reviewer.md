# reviewer.py Documentation

## Overview

`reviewer.py` implements **Stage 2** of the Multi-LLM Collaborative Debate System: Peer Review. In this stage, each of the three solvers reviews the solutions generated by the other two solvers, providing structured feedback.

## Purpose

After initial solutions are generated in Stage 1, solvers evaluate each other's work to:
- Identify strengths and weaknesses
- Detect errors in reasoning or calculations
- Provide constructive suggestions
- Assess answer correctness
- Build consensus or highlight disagreements

## Main Functions

### `review_solution(problem_text, reviewer_solution, solution_to_review, reviewer_id, target_solver_id)`
Generates a review of one solver's solution by another solver.

**Parameters:**
- `problem_text` (str): Original problem statement
- `reviewer_solution` (dict): The reviewer's own solution (for context)
- `solution_to_review` (dict): The solution being reviewed
- `reviewer_id` (str): ID of the solver doing the review (e.g., `"solver_1"`)
- `target_solver_id` (str): ID of the solver whose solution is being reviewed

**Returns:**
- Dictionary containing:
  - `reviewer_id`: ID of the reviewer
  - `reviewer_model`: Model used by reviewer
  - `target_solver_id`: ID of solution being reviewed
  - `target_model`: Model used by target solver
  - `strengths`: List of identified strengths
  - `weaknesses`: List of identified weaknesses
  - `errors`: List of errors found (if any)
  - `suggestions`: List of improvement suggestions
  - `overall_assessment`: Overall evaluation text
  - `answer_correctness`: `"correct"`, `"incorrect"`, or `"uncertain"`
  - `confidence`: Confidence score (0.0-1.0)
  - `raw_response`: Full LLM response

### `review_solutions(problem_id, problem_text, solutions)`
Main function that orchestrates all peer reviews.

**Parameters:**
- `problem_id` (str): Unique problem identifier
- `problem_text` (str): Problem statement
- `solutions` (dict): Dictionary from Stage 1 with all three solutions

**Returns:**
- Dictionary with all reviews:
  ```python
  {
      "solver_1": {
          "review_of_solver_2": {...},
          "review_of_solver_3": {...}
      },
      "solver_2": {
          "review_of_solver_1": {...},
          "review_of_solver_3": {...}
      },
      "solver_3": {
          "review_of_solver_1": {...},
          "review_of_solver_2": {...}
      }
  }
  ```

**Output File:**
- Saves to: `data/raw_outputs/{problem_id}_stage2_reviews.json`

## Review Process

1. **Context Setting**: Reviewer sees their own solution and the target solution
2. **Analysis**: Reviewer evaluates reasoning, approach, and answer
3. **Feedback Generation**: Structured feedback with strengths, weaknesses, errors, suggestions
4. **Correctness Assessment**: Reviewer judges if the answer is correct

## Review Structure

Each review contains:
- **Strengths**: What the solution does well
- **Weaknesses**: Areas that need improvement
- **Errors**: Specific mistakes found
- **Suggestions**: Concrete improvement recommendations
- **Overall Assessment**: High-level evaluation
- **Answer Correctness**: Binary/ternary judgment
- **Confidence**: How confident the reviewer is

## LLM Prompt Structure

The review prompt includes:
- Original problem statement
- Reviewer's own solution (for comparison)
- Target solution to review (reasoning steps, answer, approach)
- Instructions for thorough, constructive review

## API Calls

- **6 API calls** total (each solver reviews 2 others)
- Uses each solver's original model from Stage 1
- Temperature: `TEMPERATURES["reviewer"]` (from config)
- Response format: JSON object

## Error Handling

- **Missing solutions**: Raises `ValueError` if solver not found
- **API errors**: Retries with exponential backoff (via `call_llm`)
- **JSON parsing**: Handles markdown-wrapped JSON responses

## Dependencies

### Internal
- `config.py`: API key, temperatures, paths, retry settings

### External
- `openai`: OpenAI API client

### Standard Library
- `json`: JSON parsing
- `os`: File operations
- `time`: Rate limiting

## Example Output Structure

```json
{
  "problem_id": "math_001",
  "problem_text": "...",
  "reviews": {
    "solver_1": {
      "review_of_solver_2": {
        "reviewer_id": "solver_1",
        "target_solver_id": "solver_2",
        "strengths": ["Clear reasoning", "Correct approach"],
        "weaknesses": ["Missing edge case"],
        "errors": ["Calculation error in step 3"],
        "suggestions": ["Check step 3 calculation", "Consider edge case"],
        "overall_assessment": "Good approach but has calculation error",
        "answer_correctness": "incorrect",
        "confidence": 0.85
      },
      "review_of_solver_3": {...}
    },
    "solver_2": {...},
    "solver_3": {...}
  },
  "timestamp": "2024-01-01 12:00:00"
}
```

## Integration

- **Input**: Requires Stage 1 solutions
- **Output**: Used by Stage 3 (refinement) to guide solution improvement
- **Called by**: `orchestrator.py` as part of the full pipeline

## Standalone Usage

Can be run independently for testing:

```bash
python code/reviewer.py
```

This will:
1. Load the first problem from the dataset
2. Load existing Stage 1 solutions
3. Generate all peer reviews
4. Save results to `data/raw_outputs/`

## Key Features

- **Structured Feedback**: Consistent format across all reviews
- **Error Detection**: Explicit identification of errors
- **Constructive**: Focuses on actionable suggestions
- **Confidence Scoring**: Quantifies reviewer certainty
- **Answer Assessment**: Binary/ternary correctness judgment
